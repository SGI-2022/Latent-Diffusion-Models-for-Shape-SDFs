{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6928f68f-9ab8-44ea-879b-0699f443424d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Load clsses/functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3489bac3-f8ae-4883-bafc-194340f9f819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "from skimage import measure\n",
    "import meshplot as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import random\n",
    "import math\n",
    "from itertools import chain as chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b488b323-2c81-4797-8f65-006d349f1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChairDataset(Dataset):\n",
    "    def __init__(self, file_paths, n_points_to_load):\n",
    "        self.file_paths = file_paths\n",
    "        self.n_points_per_shape = 50000\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            training_set = np.load(file_path)\n",
    "            assert len(training_set['points']) == self.n_points_per_shape, f\"{self.n_points_per_shape} data points expected, got: {training_set['points']}\"\n",
    "                \n",
    "        self.n_points_to_load = n_points_to_load # number of points to load at once \n",
    "        \n",
    "    def __getitem__(self, shape_idx):        \n",
    "        training_set = np.load(self.file_paths[shape_idx]) # TODO: try mmap_mode='r'\n",
    "        points = training_set['points']\n",
    "        sdfs = training_set['sdf']\n",
    "        \n",
    "        n_shape_idx = np.full((self.n_points_to_load, 1), shape_idx, dtype=int)\n",
    "        \n",
    "        n_rand = random.sample(range(self.n_points_per_shape), self.n_points_to_load) # randomly pick 'n_points_to_load number' of indices  \n",
    "        \n",
    "        n_points = points[n_rand]\n",
    "        \n",
    "        n_sdf = sdfs[n_rand]\n",
    "        n_sdf = np.resize(n_sdf, (self.n_points_to_load, 1))\n",
    "        \n",
    "        return n_shape_idx, n_points, n_sdf\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74f4f948-1da7-4473-aa7e-cbc41a92912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(all_file_or_not, n_files = 0):\n",
    "    file_paths = []\n",
    "    main_dir = '../data/03001627_sdfs/'\n",
    "\n",
    "    if all_file_or_not: # loading all files\n",
    "        n_files = 0\n",
    "        for sub_dir in os.scandir(main_dir):\n",
    "            if sub_dir.is_dir():\n",
    "                for file in os.listdir(main_dir + sub_dir.name):\n",
    "                    file_paths.append(main_dir + sub_dir.name + '/' + file) if file.endswith(\"sdf_samples.npz\") else None\n",
    "            n_files += 1\n",
    "            \n",
    "    else: # loading specific # of files\n",
    "        for sub_dir in os.scandir(main_dir):\n",
    "            if sub_dir.is_dir():\n",
    "                for file in os.listdir(main_dir + sub_dir.name):\n",
    "                    file_paths.append(main_dir + sub_dir.name + '/' + file) if file.endswith(\"sdf_samples.npz\") else None\n",
    "            if len(file_paths) == n_files:\n",
    "                break\n",
    "    \n",
    "    print(f'total # of files: {n_files}')\n",
    "    return file_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fa72699-7834-4639-9b3a-c3c579f3c567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_shapes, shape_code_length, n_inner_nodes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.shape_code_length = shape_code_length\n",
    "        self.shape_codes = nn.Embedding(n_shapes, shape_code_length) # shape code as an embedding\n",
    "        nn.init.normal_(self.shape_codes.weight, mean=0, std=0.01)\n",
    "        \n",
    "        self.linear1 = nn.Linear(3 + shape_code_length, n_inner_nodes) # (x, y, z) + shape code \n",
    "        self.linear2 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear3 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear4 = nn.Linear(n_inner_nodes, n_inner_nodes - (3 + shape_code_length))\n",
    "        self.linear5 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear6 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear7 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear8 = nn.Linear(n_inner_nodes, 1) # output a SDF value\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, shape_idx, x):\n",
    "        shape_code = self.shape_codes(shape_idx.view(1, -1))\n",
    "        shape_code = shape_code.view(-1, self.shape_code_length)\n",
    "        shape_code_with_xyz = torch.cat((x, shape_code), dim=1) # concatenate horizontally\n",
    "        \n",
    "        out = self.linear1(shape_code_with_xyz)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear5(torch.cat((out, shape_code_with_xyz), dim=1)) # skip connection\n",
    "        out = self.relu(out)\n",
    "        out = self.linear6(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear7(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear8(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def add_noise_to_shape_codes(self, beta):\n",
    "        self.shape_codes.weight.data += beta*(torch.rand_like(self.shape_codes.weight) * torch.std(self.shape_codes.weight, unbiased=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6188ca0c-8ad9-468d-b867-d03e52a571f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_points(file_paths, n_points_per_shape, n_points_to_generate):\n",
    "    shape_idx = random.sample(range(len(file_paths)), 1)[0] # pick a shape randomly out of 7000 shapes\n",
    "    n_shape_idx = np.full((n_points_to_generate, 1), shape_idx, dtype=int)\n",
    "    \n",
    "    training_set = np.load(file_paths[shape_idx]) \n",
    "    points = training_set['points']\n",
    "    sdfs = training_set['sdf']\n",
    "\n",
    "    rand = random.sample(range(n_points_per_shape), n_points_to_generate) # pick 1000 points randomly\n",
    "\n",
    "    return torch.from_numpy(n_shape_idx), torch.from_numpy(points[rand]), torch.from_numpy(sdfs[rand])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ec57306-69b1-4237-8873-aba7d755db6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total # of files: 6778\n"
     ]
    }
   ],
   "source": [
    "file_paths = load_files(True)\n",
    "\n",
    "n_points_per_shape = 50000\n",
    "n_points_to_load = 2048  # n_points_to_load= n points loaded at once from a single file\n",
    "batch_size = 10 # batch_size = n shapes loaded in one batch, not n data points\n",
    "dataset = ChairDataset(file_paths=file_paths, n_points_to_load=n_points_to_load)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111fee15-cc5b-48ea-8a40-5de2bbcaef58",
   "metadata": {},
   "source": [
    "#### Plotting inferred shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa7cbc9-7023-42e2-ae21-b94136186321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = './models/autodecoder_08052022_073446' # loading one point at a time\n",
    "# model = MLP_old(10, 256, 256)\n",
    "# filename = './models/autodecoder_08122022_202902' # loading multiple points at a time, 8 layers\n",
    "# model = MLP(10, 256, 512)\n",
    "# filename = './models/autodecoder_08122022_234636' # loading multiple points at a time, 3 layers, extra permutation\n",
    "# model = MLP_old(10, 256, 256)\n",
    "# filename = './models/autodecoder_08132022_003242' # loading multiple points at a time, 4 layers, extra permutation, \n",
    "# model = MLP_old(10, 256, 256)\n",
    "# filename = './models/autodecoder_08132022_011401' # loading multiple points at a time, 4 layers, extra permutation, rigor=5 \n",
    "# model = MLP_old(10, 256, 256)\n",
    "# filename = './models/autodecoder_08132022_041952' # loading multiple points at a time, 8 layers, 16 shapes, extra permutation, rigor=5 \n",
    "# model = MLP(16, 256, 512)\n",
    "# filename = './models/autodecoder_08132022_170026' # loading multiple points at a time, 8 layers, 16 shapes, extra permutation, rigor=2, w.o clamping \n",
    "# model = MLP(16, 256, 512)\n",
    "# filename = './models/autodecoder_08132022_190738' # loading 1024 points at a time, 8 layers, 6778 shapes, extra permutation, up to epoch 1813, without clamping\n",
    "# model = MLP(6778, 256, 512)\n",
    "# filename = './models/autodecoder_08142022_121303 # loading 1024 points at a time, 8 layers, 6778 shapes, extra permutation, up to epoch 1813, clamping + L1loss\n",
    "# model = MLP(6778, 256, 512)\n",
    "# 08142022_231250 # tanh, training loss increased\n",
    "# model = MLP(6778, 256, 512)\n",
    "# 08182022_105030 # remove max_norm, initialization only, regularization param: sigma = 1e2\n",
    "# model = MLP(6778, 256, 512)\n",
    "# 08182022_105028 # remove max_norm, initialization only, regularization param: sigma = 1e-2\n",
    "# model = MLP(6778, 256, 512)\n",
    "# 08182022_172451 # rerun on 10 shapes with the most updated network\n",
    "# model = MLP(10, 256, 256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3ea869c-7667-43dc-abdd-a104434c2c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c19826e7-58e9-436d-a0de-268405bae71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = './models/autodecoder_08182022_200131_28'\n",
    "# model = MLP(6778, 256, 512)\n",
    "# model.load_state_dict(torch.load(filename))\n",
    "# model.eval()\n",
    "\n",
    "# print(model.shape_codes.weight[0][:5])\n",
    "# print(torch.std(model.shape_codes.weight))\n",
    "# model.shape_codes.weight.data=0.1*torch.rand_like(model.shape_codes.weight) * torch.std(model.shape_codes.weight)\n",
    "# print(model.shape_codes.weight[0][:5])\n",
    "                                  \n",
    "                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34f30726-f34e-430b-87a8-1c94a2d54c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d604a0804f4066be215d9c155b0ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(50.779643…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a8b65cd73a4421888899df1b264f5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(48.985365…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5311d75c4f144119990336aaa8dca232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(46.501544…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2466a5abbf1649febd7f4cce788823d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(48.517990…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7fcb618b6c70>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename = './models/autodecoder_08182022_200131_86'\n",
    "filename = './models/autodecoder_08182022_181516_91'\n",
    "\n",
    "\n",
    "model = MLP(6778, 256, 512)\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.eval()\n",
    "\n",
    "shape_idx = 0\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)\n",
    "\n",
    "\n",
    "shape_idx = 1\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)\n",
    "\n",
    "shape_idx = 3000\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)\n",
    "\n",
    "shape_idx = 6000\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bd4ed-deae-4aeb-900f-1a8831a8b2e2",
   "metadata": {},
   "source": [
    "#### plotting ground truth shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86082f1c-a052-4cba-a908-0591866d31e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulaugguerrero_gmail_com/miniconda3/lib/python3.9/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float32\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  warnings.warn(\n",
      "/home/paulaugguerrero_gmail_com/miniconda3/lib/python3.9/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"uint32\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298f8c0bb16845368494216705be4387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0, 0.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34282533dd7747759c0d7a5c0a3daf4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0, 0.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ace447bd68f84e42a156eeb0f652df87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0, 0.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e02d03ac3324039ba80f29b0abd22d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0, 0.0,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7fcb621d1460>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_idx = 0\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))\n",
    "\n",
    "shape_idx = 1\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))\n",
    "\n",
    "shape_idx = 3000\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))\n",
    "\n",
    "shape_idx = 6000\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "436768dd-2be8-4327-8313-5073c843c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for shape_idx in range(10):\n",
    "#     mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "#     mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba2e4ed-b6ba-4f09-bb3c-6ce7fea1877f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
