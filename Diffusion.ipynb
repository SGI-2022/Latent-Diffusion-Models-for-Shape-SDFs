{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7692e827-bb36-4fd5-95f0-5586616124c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "from skimage import measure\n",
    "import meshplot as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta, datetime\n",
    "import random\n",
    "import math\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3f88105b-0e7e-46d5-a029-6f3a6e8a5cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 256])\n"
     ]
    }
   ],
   "source": [
    "# 4 layer autodecoder MLP class for 10 shapes \n",
    "class MLP_4(nn.Module):\n",
    "    def __init__(self, n_shapes, shape_code_length, n_inner_nodes):\n",
    "        super(MLP_4, self).__init__()\n",
    "        self.shape_code_length = shape_code_length\n",
    "        self.shape_codes = nn.Embedding(n_shapes, shape_code_length, max_norm=0.01) # shape code as an embedding # TODO: take this outside \n",
    "        \n",
    "        self.linear1 = nn.Linear(3 + shape_code_length, n_inner_nodes) # (x, y, z) + shape code \n",
    "        self.linear2 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear3 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear4 = nn.Linear(n_inner_nodes, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, shape_idx, x):\n",
    "        shape_code = self.shape_codes(shape_idx.view(1, -1))\n",
    "        print(shape_code.shape)\n",
    "        shape_code = shape_code.view(-1, self.shape_code_length)\n",
    "        shape_code_with_xyz = torch.cat((x, shape_code), dim=1) # concatenate horizontally\n",
    "        \n",
    "        out = self.linear1(shape_code_with_xyz)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear4(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "filename = './models/autodecoder_08052022_073446' # trained for 10 shapes\n",
    "model = MLP_4(10, 256, 256)\n",
    "model.load_state_dict(torch.load(filename))\n",
    "\n",
    "shape_codes = model.state_dict()['shape_codes.weight']\n",
    "print(shape_codes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ad27ed2f-f79d-4635-862d-e41bab751d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_codes = np.zeros((100,2))\n",
    "for i in range(1,100):\n",
    "    shape_codes[i,: ] =[(shape_codes[i-1,0]+1),(shape_codes[i-1,0]+1)]\n",
    "shape_codes1 = np.zeros((100,2))\n",
    "for i in range(1,100):\n",
    "    shape_codes1[i,: ] =[(shape_codes1[i-1,0]-1),(-shape_codes1[i-1,0]+1)+100]\n",
    "shape_codes = np.array([shape_codes/10,shape_codes1/10])\n",
    "#shape_codes = np.hstack((shape_codes,shape_codes))\n",
    "#shape_codes = np.hstack((shape_codes,shape_codes))\n",
    "shape_codes = torch.tensor(shape_codes)\n",
    "#shape_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba803fcb-2974-4e0b-8bb3-bba3a99f72ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderPartial(nn.Module):\n",
    "    def __init__(self,shape_code_length, n_inner_nodes):\n",
    "        super(DecoderPartial, self).__init__()\n",
    "        self.shape_code_length = shape_code_length\n",
    "        self.linear1 = nn.Linear(3 + shape_code_length, n_inner_nodes) # (x, y, z) + shape code \n",
    "        self.linear2 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear3 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear4 = nn.Linear(n_inner_nodes, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,shape_code, x):\n",
    "        shape_code = shape_code.repeat([1000000,1])\n",
    "        print(shape_code.size())\n",
    "        shape_code = shape_code.view(-1, self.shape_code_length)\n",
    "        shape_code_with_xyz = torch.cat((x, shape_code), dim=1) # concatenate horizontally\n",
    "        out = self.linear1(shape_code_with_xyz)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear4(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9c61b84-ea2e-467e-8aee-d59939821c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderPartial(\n",
       "  (linear1): Linear(in_features=259, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (linear3): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (linear4): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = DecoderPartial(256,256)\n",
    "#mod_dict = mod.state_dict()\n",
    "mod.load_state_dict(torch.load(filename),strict=False)\n",
    "mod.eval()\n",
    "#for param_tensor in mod.state_dict():\n",
    "    #print(param_tensor, \"\\t\", mod.state_dict()[param_tensor])\n",
    "# 1. filter out unnecessary keys\n",
    "#pretrained_dict = {k: v for k, v in model.iteritems() if k in model_dict}\n",
    "# 2. overwrite entries in the existing state dict\n",
    "#model_dict.load_state_dict(pretrained_dict) \n",
    "# 3. load the new state dict\n",
    "#model.load_state_dict(pretrained_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091653d-542c-4954-8b9b-b0170801f163",
   "metadata": {},
   "source": [
    "## Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "a8aaaaa7-0fb1-4edc-a53f-408395399419",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = shape_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "4aead59f-fa38-4b82-a2a0-304bf1bbb3de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 100, 2])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "3b5d7bb0-945c-4594-a33e-6d283e37a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "std = np.std(np.asarray(dataset))\n",
    "mean = np.mean(np.asarray(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "efcc6328-ad86-4204-9669-69ac50601c25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127.5"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "e5a3e02a-bff5-46a4-b9b5-e7a849f57955",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (dataset-mean)/(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1806ab6-ff70-436f-9fa6-1964ca7a30f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_beta_schedule(schedule='linear', n_timesteps=1000, start=1e-5, end=1e-2):\n",
    "    if schedule == 'linear':\n",
    "        betas = torch.linspace(start, end, n_timesteps)\n",
    "    elif schedule == \"quad\":\n",
    "        betas = torch.linspace(start ** 0.5, end ** 0.5, n_timesteps) ** 2\n",
    "    elif schedule == \"sigmoid\":\n",
    "        betas = torch.linspace(-6, 6, n_timesteps)\n",
    "        betas = torch.sigmoid(betas) * (end - start) + start\n",
    "    return betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "207a3a1c-6e39-4fa1-b617-791499206616",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "betas = make_beta_schedule(schedule='linear', n_timesteps=n_steps, start=1e-2, end=1e-1)\n",
    "alphas = 1 - betas\n",
    "alphas_prod = torch.cumprod(alphas, 0)\n",
    "alphas_prod_p = torch.cat([torch.tensor([1]).float(), alphas_prod[:-1]], 0)\n",
    "alphas_bar_sqrt = torch.sqrt(alphas_prod)\n",
    "one_minus_alphas_bar_log = torch.log(1 - alphas_prod)\n",
    "one_minus_alphas_bar_sqrt = torch.sqrt(1 - alphas_prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07125289-8791-4395-b849-bc0a1ca84f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(input, t, x):\n",
    "    shape = x.shape\n",
    "    out = torch.gather(input, 0, t.to(input.device))\n",
    "    reshape = [t.shape[0]] + [1] * (len(shape) - 1)\n",
    "    return out.reshape(*reshape)\n",
    "def q_sample(x_0, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    alphas_t = extract(alphas_bar_sqrt, t, x_0)\n",
    "    alphas_1_m_t = extract(one_minus_alphas_bar_sqrt, t, x_0)\n",
    "    return (alphas_t * x_0 + alphas_1_m_t * noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53b6f568-981d-4413-9976-909044e9e8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample(model, x, t):\n",
    "    t = torch.tensor([t])\n",
    "    # Factor to the model output\n",
    "    eps_factor = ((1 - extract(alphas, t, x)) / extract(one_minus_alphas_bar_sqrt, t, x))\n",
    "    # Model output\n",
    "    eps_theta = model(x, t)\n",
    "    # Final values\n",
    "    mean = (1 / extract(alphas, t, x).sqrt()) * (x - (eps_factor * eps_theta))\n",
    "    # Generate z\n",
    "    z = torch.randn_like(x)\n",
    "    # Fixed sigma\n",
    "    sigma_t = extract(betas, t, x).sqrt()\n",
    "    sample = mean + sigma_t * z\n",
    "    return (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "710fb66e-2081-4be5-b183-722808e7fe88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mod' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# inferring\u001b[39;00m\n\u001b[1;32m      8\u001b[0m q\u001b[38;5;241m=\u001b[39mstd\u001b[38;5;241m*\u001b[39mdataset[\u001b[38;5;241m6\u001b[39m,:]\u001b[38;5;241m+\u001b[39mmean\n\u001b[0;32m----> 9\u001b[0m volume \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m(q,P)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;28mlen\u001b[39m(y), \u001b[38;5;28mlen\u001b[39m(z)) \n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# marching cube to visualize\u001b[39;00m\n\u001b[1;32m     12\u001b[0m volume \u001b[38;5;241m=\u001b[39m volume\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mod' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "# inferring\n",
    "q=std*dataset[6,:]+mean\n",
    "volume = mod(q,P).view(len(x), len(y), len(z)) \n",
    "\n",
    "# marching cube to visualize\n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)\n",
    "\n",
    "for i in range(100):\n",
    "    q_i = std*q_sample(dataset[6,:], torch.tensor([i]))+mean\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "\n",
    "# inferring\n",
    "volume = mod(q_i,P).view(len(x), len(y), len(z)) \n",
    "\n",
    "# marching cube to visualize\n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd801b1a-b89b-4e73-9227-51a885644d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_mean_coef_1 = (betas * torch.sqrt(alphas_prod_p) / (1 - alphas_prod))\n",
    "posterior_mean_coef_2 = ((1 - alphas_prod_p) * torch.sqrt(alphas) / (1 - alphas_prod))\n",
    "posterior_variance = betas * (1 - alphas_prod_p) / (1 - alphas_prod)\n",
    "posterior_log_variance_clipped = torch.log(torch.cat((posterior_variance[1].view(1, 1), posterior_variance[1:].view(-1, 1)), 0)).view(-1)\n",
    "\n",
    "def q_posterior_mean_variance(x_0, x_t, t):\n",
    "    coef_1 = extract(posterior_mean_coef_1, t, x_0)\n",
    "    coef_2 = extract(posterior_mean_coef_2, t, x_0)\n",
    "    mean = coef_1 * x_0 + coef_2 * x_t\n",
    "    var = extract(posterior_log_variance_clipped, t, x_0)\n",
    "    return mean, var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58fb6624-989b-43e3-87a5-e3f9eaeaf7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestep_embedding(timesteps, embedding_dim):\n",
    "    \"\"\"\n",
    "    This matches the implementation in Denoising Diffusion Probabilistic Models:\n",
    "    From Fairseq.\n",
    "    Build sinusoidal embeddings.\n",
    "    This matches the implementation in tensor2tensor, but differs slightly\n",
    "    from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "    \"\"\"\n",
    "    assert len(timesteps.shape) == 1\n",
    "\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float32) * -emb)\n",
    "    emb = emb.to(device=timesteps.device)\n",
    "    emb = timesteps.float()[:, None] * emb[None, :]\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6c85da4-b0c2-4d3d-90c8-549de7e5b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConditionalLinear(nn.Module):\n",
    "    def __init__(self, num_in, num_out, n_steps, temb_ch):\n",
    "        super(ConditionalLinear, self).__init__()\n",
    "        self.num_out = num_out\n",
    "        self.lin = nn.Linear(num_in, num_out)\n",
    "        # self.embed = nn.Embedding(n_steps, num_out)\n",
    "        # self.embed.weight.data.uniform_()\n",
    "\n",
    "        self.temb_proj = torch.nn.Linear(temb_ch,\n",
    "                                         num_out)\n",
    "        self.nonlin = torch.nn.SiLU()\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        out = self.lin(x)\n",
    "\n",
    "        out = self.temb_proj(self.nonlin(y)) + out\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "6be6a8a6-27dc-41f6-b0e6-a6de3b2c623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalModel(nn.Module):\n",
    "    def __init__(self, n_steps, ch=32, num_out=64):\n",
    "        super(ConditionalModel, self).__init__()\n",
    "        self.ch = ch\n",
    "        self.temb_ch = ch * 4\n",
    "        self.lin1 = ConditionalLinear(2,128, n_steps, self.temb_ch)\n",
    "        self.lin2 = ConditionalLinear(128,128, n_steps, self.temb_ch)\n",
    "        self.lin3 = nn.Linear(128,2)\n",
    "\n",
    "\n",
    "        # timestep embedding\n",
    "        self.temb = nn.Sequential(\n",
    "            torch.nn.Linear(ch,\n",
    "                            self.temb_ch),\n",
    "            torch.nn.SiLU(),\n",
    "            torch.nn.Linear(self.temb_ch,\n",
    "                            self.temb_ch),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        y = get_timestep_embedding(y, self.ch)\n",
    "        temb = self.temb(y)\n",
    "        x = F.softplus(self.lin1(x, temb))\n",
    "        x = F.softplus(self.lin2(x, temb))\n",
    "        return self.lin3(x)\n",
    "\n",
    "def p_sample(model, x, t):\n",
    "    t = torch.tensor([t])\n",
    "    # Factor to the model output\n",
    "    eps_factor = ((1 - extract(alphas, t, x)) / extract(one_minus_alphas_bar_sqrt, t, x))\n",
    "    # Model output\n",
    "    eps_theta = model(x, t)\n",
    "    # Final values\n",
    "    mean = (1 / extract(alphas, t, x).sqrt()) * (x - (eps_factor * eps_theta))\n",
    "    # Generate z\n",
    "    z = torch.randn_like(x)\n",
    "    # Fixed sigma\n",
    "    sigma_t = extract(betas, t, x).sqrt()\n",
    "    sample = mean + sigma_t * z\n",
    "    return (sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5364b107-6580-4732-94eb-e7422930b26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_sample_loop(model, shape):\n",
    "    cur_x = torch.randn(shape)\n",
    "    x_seq = [cur_x]\n",
    "    for i in reversed(range(n_steps)):\n",
    "        cur_x = p_sample(model, cur_x, i)\n",
    "        x_seq.append(cur_x)\n",
    "    return x_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a4d9efa-7108-43e5-9273-37af8e6ce7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_standard_normal_cdf(x):\n",
    "    return 0.5 * (1.0 + torch.tanh(torch.tensor(np.sqrt(2.0 / np.pi)) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "def discretized_gaussian_log_likelihood(x, means, log_scales):\n",
    "    # Assumes data is integers [0, 255] rescaled to [-1, 1]\n",
    "    centered_x = x - means\n",
    "    inv_stdv = torch.exp(-log_scales)\n",
    "    plus_in = inv_stdv * (centered_x + 1. / 255.)\n",
    "    cdf_plus = approx_standard_normal_cdf(plus_in)\n",
    "    min_in = inv_stdv * (centered_x - 1. / 255.)\n",
    "    cdf_min = approx_standard_normal_cdf(min_in)\n",
    "    log_cdf_plus = torch.log(torch.clamp(cdf_plus, min=1e-12))\n",
    "    log_one_minus_cdf_min = torch.log(torch.clamp(1 - cdf_min, min=1e-12))\n",
    "    cdf_delta = cdf_plus - cdf_min\n",
    "    log_probs = torch.where(x < -0.999, log_cdf_plus, torch.where(x > 0.999, log_one_minus_cdf_min, torch.log(torch.clamp(cdf_delta, min=1e-12))))\n",
    "    return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "084d2135-b7c2-4fd0-9d9a-391c85042c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_variational(model, x_0):\n",
    "    batch_size = x_0.shape[0]\n",
    "    # Select a random step for each example\n",
    "    t = torch.randint(0, n_steps, size=(batch_size // 2 + 1,))\n",
    "    t = torch.cat([t, n_steps - t - 1], dim=0)[:batch_size].long()\n",
    "    # Perform diffusion for step t\n",
    "    x_t = q_sample(x_0, t)\n",
    "    # Compute the true mean and variance\n",
    "    true_mean, true_var = q_posterior_mean_variance(x_0, x_t, t)\n",
    "    # Infer the mean and variance with our model\n",
    "    model_mean, model_var = p_mean_variance(model, x_t, t)\n",
    "    # Compute the KL loss\n",
    "    kl = normal_kl(true_mean, true_var, model_mean, model_var)\n",
    "    kl = torch.mean(kl.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "    # NLL of the decoder\n",
    "    decoder_nll = -discretized_gaussian_log_likelihood(x_0, means=model_mean, log_scales=0.5 * model_var)\n",
    "    decoder_nll = torch.mean(decoder_nll.view(batch_size, -1), dim=1) / np.log(2.)\n",
    "    # At the first timestep return the decoder NLL, otherwise return KL(q(x_{t-1}|x_t,x_0) || p(x_{t-1}|x_t))\n",
    "    output = torch.where(t == 0, decoder_nll, kl)\n",
    "    return output.mean(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "79631a59-513d-470a-b4ae-ed111beb5015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 100, 2])"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b=[]\n",
    "for i in range(n_steps):\n",
    "    f=torch.randn_like(dataset[0,:,:])\n",
    "    #for k in range(2):\n",
    "    b.append(f[None,:,:])\n",
    "e = torch.Tensor(n_steps,100, 2)\n",
    "torch.cat(b, out=e)\n",
    "e.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "44706245-d288-4cee-88b1-b6bd5e3f5d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_estimation_loss(model, x_0,noise_steps,h):\n",
    "    batch_size = x_0.shape[0]\n",
    "    # Select a random step for each example\n",
    "    t = torch.randint(0,n_steps, size=(batch_size//2+1,)) # pick (batch_size//2+1) number of rand integers in bw 0 and n_steps\n",
    "    t = torch.cat([t, n_steps - t - 1], dim=0)[:batch_size].long() # pick other time index symmetrically\n",
    "    # x0 multiplier\n",
    "    a = extract(alphas_bar_sqrt, t, x_0)\n",
    "    # eps multiplier\n",
    "    am1 = extract(one_minus_alphas_bar_sqrt, t, x_0)\n",
    "    #e = torch.randn_like(x_0)\n",
    "    e = noise_steps[t,:,:]\n",
    "    # model input\n",
    "    x = x_0 * a + e * am1\n",
    "    output = model(x, t)\n",
    "    loss = (e - output).square().mean()\n",
    "    if loss>0.15 and h>50000:\n",
    "        print(t)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "965efe20-f64e-41ba-8f28-8cf40153de46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMA(object):\n",
    "    def __init__(self, mu=0.999):\n",
    "        self.mu = mu\n",
    "        self.shadow = {}\n",
    "\n",
    "    def register(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name].data = (1. - self.mu) * param.data + self.mu * self.shadow[name].data\n",
    "\n",
    "    def ema(self, module):\n",
    "        for name, param in module.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                param.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    def ema_copy(self, module):\n",
    "        module_copy = type(module)(module.config).to(module.config.device)\n",
    "        module_copy.load_state_dict(module.state_dict())\n",
    "        self.ema(module_copy)\n",
    "        return module_copy\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.shadow\n",
    "\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.shadow = state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "50ae40e1-f17f-4246-90be-7d5c182057f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConditionalModel(n_steps)\n",
    "optimizer = torch.optim.Adam(model.parameters(),betas = (0.9, 0.9),\n",
    "eps = 1e-08 * 10, lr=1e-3)\n",
    "#dataset = torch.tensor(data.T).float()\n",
    "dataset = dataset.float()\n",
    "# Create EMA model\n",
    "ema = EMA(0.9)\n",
    "ema.register(model)\n",
    "# Batch size\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "602b43b9-597c-4a8c-8eab-4f19abec20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename = 'log_file_new.log',\n",
    "                level = logging.DEBUG,\n",
    "                format = '%(asctime)s:%(levelname)s:%(name)s:%(message)s')\n",
    "logging.debug(\"Epoch  Loss \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "c470f3e4-46f1-4ed4-8e2b-4d092e0f0ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0490, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0089, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0010, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [380]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Calling the step function to update the parameters\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Update the exponential moving average\u001b[39;00m\n\u001b[1;32m     19\u001b[0m ema\u001b[38;5;241m.\u001b[39mupdate(model)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    107\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    155\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 157\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m         \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m         \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m         \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m         \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m         \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m         \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m         \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m         \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m         \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m         \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m         \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    211\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 213\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/optim/adam.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    264\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 265\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[1;32m    268\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(10000):\n",
    "    # X is a torch Variable\n",
    "    permutation = torch.randperm(dataset.size()[0])\n",
    "    for i in range(0, dataset.size()[0], batch_size):\n",
    "        # Retrieve current batch\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x = dataset[indices]\n",
    "        # Compute the loss.\n",
    "        loss = noise_estimation_loss(model, batch_x,e,t)\n",
    "        # Before the backward pass, zero all of the network gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Backward pass: compute gradient of the loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Perform gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        # Calling the step function to update the parameters\n",
    "        optimizer.step()\n",
    "        # Update the exponential moving average\n",
    "        ema.update(model)\n",
    "    # Print loss\n",
    "    #logging.debug(\"Epoch %d Loss %f\",t,loss)\n",
    "    if (t % 1000 == 0):\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf820c6f-e6b0-4eb7-81a9-073a912060e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare points to use in inferring\n",
    "x = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "64ce34d3-4826-44c0-a303-8e091b40626c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0043, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e3b8e551-4133-486d-b725-ec8d2a733966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f982880ca90>"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_0 = dataset[1,:,:]\n",
    "#q=std*x_0[0,:]+mean\n",
    "#volume = mod(q,P).view(len(x), len(y), len(z)) \n",
    "# marching cube to visualize\n",
    "#volume = volume.detach().numpy()\n",
    "#verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "#mp.plot(verts, faces)\n",
    "t=torch.tensor([999])\n",
    "a = extract(alphas_bar_sqrt, t, x_0)\n",
    "# eps multiplier\n",
    "am1 = extract(one_minus_alphas_bar_sqrt, t, x_0)\n",
    "#e = torch.randn_like(x_0)\n",
    "# model input\n",
    "#e.size()\n",
    "xx = x_0 * a + e[999,:,:] * am1\n",
    "#q=std*xx[0,:]+mean\n",
    "#volume = mod(q,P).view(len(x), len(y), len(z)) \n",
    "\n",
    "# marching cube to visualize\n",
    "#volume = volume.detach().numpy()\n",
    "#verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "#mp.plot(verts, faces)\n",
    "\n",
    "#output = model(x, t)\n",
    "cur_x = xx\n",
    "for i in reversed(range(1000)):\n",
    "        cur_x = p_sample(model, cur_x, i)\n",
    "    #x_seq.append(cur_x)\n",
    "#q=std*cur_x[0,:]+mean\n",
    "#volume = mod(q,P).view(len(x), len(y), len(z)) \n",
    "\n",
    "# marching cube to visualize\n",
    "#volume = volume.detach().numpy()\n",
    "#verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "#mp.plot(verts, faces)\n",
    "#print(x_0)\n",
    "#print(\"*****\")\n",
    "#print(xx)\n",
    "#print(\"*****\")\n",
    "#print(cur_x)\n",
    "cur_x=cur_x.detach().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(x_0[:,0],x_0[:,1])\n",
    "plt.scatter(dataset[:,:,0],dataset[:,:,1])\n",
    "plt.scatter(xx[:,0],xx[:,1])\n",
    "plt.scatter(cur_x[:,0],cur_x[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db18b81-f4ed-4dfc-9e0b-394b2b33e5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 256])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d6c1a7aeb7453d9d415ee1a2eb3e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(49.827744…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 256])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96b628a877c441a98704390abe06916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(49.882847…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000000, 256])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d586030b23442a3aced59a4f9780699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(49.504834…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7f99244addc0>"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0 = dataset[:5,:]\n",
    "q=std*x_0[0,:]+mean\n",
    "volume = mod(q,P).view(len(x), len(y), len(z)) \n",
    "# marching cube to visualize\n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)\n",
    "t=torch.tensor([600])\n",
    "a = extract(alphas_bar_sqrt, t, x_0)\n",
    "# eps multiplier\n",
    "am1 = extract(one_minus_alphas_bar_sqrt, t, x_0)\n",
    "#e = torch.randn_like(x_0)\n",
    "# model input\n",
    "xx = x_0 * a + e * am1\n",
    "q=std*xx[0,:]+mean\n",
    "volume = mod(q,P).view(len(x), len(y), len(z)) \n",
    "\n",
    "# marching cube to visualize\n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)\n",
    "\n",
    "#output = model(x, t)\n",
    "cur_x = xx\n",
    "for i in reversed(range(600)):\n",
    "    #for j in range(10):\n",
    "    cur_x = p_sample(model, cur_x, i)\n",
    "    #x_seq.append(cur_x)\n",
    "q=std*cur_x[0,:]+mean\n",
    "volume = mod(q,P).view(len(x), len(y), len(z)) \n",
    "\n",
    "# marching cube to visualize\n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc648a7c-781e-4aa1-a4db-d7a802760e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(all_file_or_not, n_files = 0):\n",
    "    file_paths = []\n",
    "    main_dir = '../data/03001627_sdfs/'\n",
    "\n",
    "    if all_file_or_not: # loading all files\n",
    "        n_files = 0\n",
    "        for sub_dir in os.scandir(main_dir):\n",
    "            if sub_dir.is_dir():\n",
    "                for file in os.listdir(main_dir + sub_dir.name):\n",
    "                    file_paths.append(main_dir + sub_dir.name + '/' + file) if file.endswith(\"sdf_samples.npz\") else None\n",
    "            n_files += 1\n",
    "            \n",
    "    else: # loading specific # of files\n",
    "        for sub_dir in os.scandir(main_dir):\n",
    "            if sub_dir.is_dir():\n",
    "                for file in os.listdir(main_dir + sub_dir.name):\n",
    "                    file_paths.append(main_dir + sub_dir.name + '/' + file) if file.endswith(\"sdf_samples.npz\") else None\n",
    "            if len(file_paths) == n_files:\n",
    "                break\n",
    "    \n",
    "    print(f'total # of files: {n_files}')\n",
    "    return file_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf2026b-6246-4976-9316-567bd09d3f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000000, 256])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002d0959dfc84218a031d127125dabb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(49.495206…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7f9924ff77c0>"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape_idx = 8 #### change this ####\n",
    "\n",
    "# load model\n",
    "filename = './models/autodecoder_08052022_073446' # trained for 10 shapes\n",
    "model = MLP_4(10, 256, 256)\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.state_dict()\n",
    "#model.state_dict()['shape_codes.weight'].copy_()####new shape code from the diffusion model###)\n",
    "model.eval()\n",
    "\n",
    "# prepare points to use in inferring\n",
    "x = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 100, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "\n",
    "# inferring\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "\n",
    "# marching cube to visualize\n",
    "volume = volume.detach().numpy()\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)\n",
    "\n",
    "# load shape files\n",
    "#file_paths = load_files(all_file_or_not=False, n_files = 10)\n",
    "\n",
    "# compare with ground truth\n",
    "#mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "#mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad244b-3858-4175-b9a8-c919fdfb3753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
