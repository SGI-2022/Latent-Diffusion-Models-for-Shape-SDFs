{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51826e46-bf48-4fe9-8d2f-0dbfd4df75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import trimesh\n",
    "from skimage import measure\n",
    "import meshplot as mp\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ce864d3-5412-4b6c-a8bc-4f9413f34bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tag shape index to each x=xyz, y=sdf\n",
    "# horizontally concatenate for multiple chapes\n",
    "# returns shape index, x=xyz (1 by 3), y=sdf\n",
    "class ChairDataset(Dataset):\n",
    "    def __init__(self, file_paths):\n",
    "        for idx, file_path in enumerate(file_paths): #TODO: move to getitem for autoencoder\n",
    "            training_set = np.load(file_path)\n",
    "            points = training_set['points']\n",
    "            points = torch.from_numpy(points.astype(np.float32)) \n",
    "            idx_tensor = torch.ones((points.shape[0], 1), dtype=torch.int) * (idx) # 0 based indexing shapes\n",
    "            \n",
    "            sdf = training_set['sdf']\n",
    "            sdf = torch.from_numpy(sdf.astype(np.float32)) \n",
    "            sdf = sdf.view(sdf.shape[0], 1)\n",
    "            \n",
    "            n_samples = sdf.shape[0] \n",
    "\n",
    "            if idx == 0:\n",
    "                self.shape_idx = idx_tensor\n",
    "                self.x = points\n",
    "                self.y = sdf\n",
    "                self.n_samples = n_samples\n",
    "            else:\n",
    "                self.shape_idx = torch.cat((self.shape_idx, idx_tensor), dim = 0) # concatenate vertically\n",
    "                self.x = torch.cat((self.x, points), dim = 0) #concatenate vertically\n",
    "                self.y = torch.cat((self.y, sdf), dim = 0) #concatenate vertically\n",
    "                self.n_samples += n_samples\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.shape_idx[index], self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d89dc8f-068b-4005-ba8b-08a304a77b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autodecoder MLP class\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, n_shapes, shape_code_length, n_inner_nodes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.shape_code_length = shape_code_length\n",
    "        self.shape_codes = nn.Embedding(n_shapes, shape_code_length, max_norm=0.01) # shape code as an embedding # TODO: take this outside \n",
    "        self.linear1 = nn.Linear(3 + shape_code_length, n_inner_nodes) # (x, y, z) + shape code \n",
    "        self.linear2 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear3 = nn.Linear(n_inner_nodes, n_inner_nodes)\n",
    "        self.linear4 = nn.Linear(n_inner_nodes, 1) # output a SDF value\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, shape_idx, x):\n",
    "        shape_code = self.shape_codes(shape_idx.view(1, -1))\n",
    "        shape_code = shape_code.view(-1, self.shape_code_length)\n",
    "        shape_code_with_xyz = torch.cat((x, shape_code), dim=1) # concatenate horizontally\n",
    "        \n",
    "        out = self.linear1(shape_code_with_xyz)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear4(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c527b3ab-168b-48c2-9651-6855b147afcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/03001627_sdfs/4e664dae1bafe49f19fb4103277a6b93/sdf_samples.npz', '../data/03001627_sdfs/65840c85162994d990de7d30a74bbb6b/sdf_samples.npz', '../data/03001627_sdfs/c7ae4cc12a7bc2581fa16f9a5527bb27/sdf_samples.npz', '../data/03001627_sdfs/e9e224bc0a0787d8320f10afdfbaa18/sdf_samples.npz', '../data/03001627_sdfs/68c7f82dd1e1634d9338458f802f5ad7/sdf_samples.npz', '../data/03001627_sdfs/58a7b826ed562b7bb0957d845ac33749/sdf_samples.npz', '../data/03001627_sdfs/e4c866b5dd958cd0803d0f5bac2abe4c/sdf_samples.npz', '../data/03001627_sdfs/9a91a491a9e74ab132c074e5313866f2/sdf_samples.npz', '../data/03001627_sdfs/670b6b7d3fe6e4a77c5a5393686fdcfc/sdf_samples.npz', '../data/03001627_sdfs/2d701c588b3bbbc458c88d30f502a452/sdf_samples.npz']\n"
     ]
    }
   ],
   "source": [
    "# loading files\n",
    "n_files = 10 \n",
    "file_paths = []\n",
    "main_dir = '../data/03001627_sdfs/'\n",
    "for sub_dir in os.scandir(main_dir):\n",
    "    if sub_dir.is_dir():\n",
    "        for file in os.listdir(main_dir + sub_dir.name):\n",
    "            file_paths.append(main_dir + sub_dir.name + '/' + file) if file.endswith(\"sdf_samples.npz\") else None\n",
    "    if len(file_paths) == n_files:\n",
    "        break\n",
    "\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4895d5d8-1f33-4b55-b59d-4649f65db3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training main loop\n",
    "dataset = ChairDataset(file_paths)\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=2048, shuffle=True) #TODO: can specify num_workers\n",
    "\n",
    "n_epochs = 1000\n",
    "learning_rate = 0.001\n",
    "# hidden_size = 500 \n",
    "# hidden_layer_act_func = \"ReLU\"\n",
    "# output_layer_act_func = None\n",
    "\n",
    "model = MLP(len(file_paths), 256, 256)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "model.train(True) # prep model for training, needed for modules like Dropout\n",
    "\n",
    "now = datetime.now() \n",
    "now = now.strftime(\"%m%d%Y_%H%M%S\")\n",
    "print(f'datetime now: {now}')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = 0.0 # monitor training loss\n",
    "\n",
    "    for shape_idx, xyz, sdf in dataloader:\n",
    "        optimizer.zero_grad() \n",
    "        sdf_pred = model(shape_idx, xyz) \n",
    "        loss = criterion(torch.clamp(sdf_pred, -0.1, 0.1), torch.clamp(sdf, -0.1, 0.1)) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()*shape_idx.size(0) # update running training loss\n",
    "        \n",
    "    train_loss = train_loss/len(dataloader.dataset) # calculate average loss over one epoch\n",
    "    if epoch%10 == 0:\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        torch.save(model.state_dict(), f'./models/autodecoder_' + now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e86457-4d9e-42b2-ba25-55c5e2e6af1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filename = './models/multiple shapes_08052022_073446'\n",
    "model = MLP(len(file_paths), 256, 256)\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.eval()\n",
    "\n",
    "x = np.linspace(-1, 1, 200, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 200, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 200, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx = 0\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e47b129-b791-4808-9270-103045306739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8242408\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m verts_combined \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     56\u001b[0m faces_combined \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m verts, faces \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap(f, batches):\n\u001b[1;32m     59\u001b[0m     faces_combined\u001b[38;5;241m.\u001b[39mextend(np\u001b[38;5;241m.\u001b[39marray(faces) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(verts_combined))\n\u001b[1;32m     60\u001b[0m     verts_combined\u001b[38;5;241m.\u001b[39mextend(np\u001b[38;5;241m.\u001b[39marray(verts))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py:870\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m_worker\u001b[0;34m(job)\u001b[0m\n\u001b[1;32m     26\u001b[0m P \u001b[38;5;241m=\u001b[39m _cartesian_product(X, Y, Z)\n\u001b[1;32m     27\u001b[0m shape_idx_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((P\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint) \u001b[38;5;241m*\u001b[39m shape_idx\n\u001b[0;32m---> 28\u001b[0m volume \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape_idx_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(X), \u001b[38;5;28mlen\u001b[39m(Y), \u001b[38;5;28mlen\u001b[39m(Z)))\n\u001b[1;32m     29\u001b[0m verts, faces, normals, values \u001b[38;5;241m=\u001b[39m measure\u001b[38;5;241m.\u001b[39mmarching_cubes(volume, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# verts = verts[faces]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, shape_idx, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m shape_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_codes(shape_idx\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     14\u001b[0m shape_code \u001b[38;5;241m=\u001b[39m shape_code\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_code_length)\n\u001b[0;32m---> 15\u001b[0m shape_code_with_xyz \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape_code\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# concatenate horizontally\u001b[39;00m\n\u001b[1;32m     17\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear1(shape_code_with_xyz)\n\u001b[1;32m     18\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "filename = './models/autodecoder_08052022_073446'\n",
    "model = MLP(len(file_paths), 256, 256)\n",
    "model.load_state_dict(torch.load(filename))\n",
    "model.eval()\n",
    "shape_idx = 0\n",
    "\n",
    "WORKERS = multiprocessing.cpu_count()\n",
    "\n",
    "def _cartesian_product(*arrays):\n",
    "    la = len(arrays)\n",
    "    dtype = np.result_type(*arrays)\n",
    "    arr = np.empty([len(a) for a in arrays] + [la], dtype=dtype)\n",
    "    for i, a in enumerate(np.ix_(*arrays)):\n",
    "        arr[...,i] = a\n",
    "    return arr.reshape(-1, la)\n",
    "\n",
    "def _worker(job):\n",
    "    X, Y, Z = job\n",
    "    # P = np.vstack(np.meshgrid(X,Y,Z)).reshape(3,-1).T\n",
    "    P = _cartesian_product(X, Y, Z)\n",
    "    P = torch.from_numpy(P)\n",
    "\n",
    "    shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * shape_idx\n",
    "    volume = model(shape_idx_tensor, P).reshape((len(X), len(Y), len(Z)))\n",
    "    volume = volume.detach().numpy()\n",
    "\n",
    "    verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "    # verts = verts[faces]\n",
    "    scale = np.array([X[1] - X[0], Y[1] - Y[0], Z[1] - Z[0]])\n",
    "    offset = np.array([X[0], Y[0], Z[0]])\n",
    "    # verts = verts * scale + offset\n",
    "    # verts = verts[faces]\n",
    "    return verts*scale+offset, faces\n",
    "\n",
    "X = np.linspace(-1, 1, 201)\n",
    "Y = np.linspace(-1, 1, 201)\n",
    "Z = np.linspace(-1, 1, 201)\n",
    "\n",
    "batch_size = 101\n",
    "s = batch_size\n",
    "Xs = [X[i:i+s+1] for i in range(0, len(X), s)]\n",
    "Ys = [Y[i:i+s+1] for i in range(0, len(Y), s)]\n",
    "Zs = [Z[i:i+s+1] for i in range(0, len(Z), s)]\n",
    "\n",
    "batches = list(itertools.product(Xs, Ys, Zs))\n",
    "num_batches = len(batches)\n",
    "num_samples = sum(len(xs) * len(ys) * len(zs) for xs, ys, zs in batches)\n",
    "print(num_batches, num_samples)\n",
    "\n",
    "pool = ThreadPool(WORKERS)\n",
    "f = partial(_worker) # _worker, sdf callables\n",
    "\n",
    "verts_combined = []\n",
    "faces_combined = []\n",
    "\n",
    "for verts, faces in pool.imap(f, batches):\n",
    "    faces_combined.extend(np.array(faces) + len(verts_combined))\n",
    "    verts_combined.extend(np.array(verts))\n",
    "    \n",
    "    \n",
    "mp.plot(np.array(verts_combined), np.array(faces_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab69b9bd-5e77-4d92-a939-2b4fd3d9058a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paulaugguerrero_gmail_com/miniconda3/lib/python3.9/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float32\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  warnings.warn(\n",
      "/home/paulaugguerrero_gmail_com/miniconda3/lib/python3.9/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"uint32\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8efc6dd9f6fb4f62b11452abafbe9fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Renderer(camera=PerspectiveCamera(children=(DirectionalLight(color='white', intensity=0.6, position=(0.0, 0.0,â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<meshplot.Viewer.Viewer at 0x7f16d785e880>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare with ground truth\n",
    "shape_idx = 0\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546ae16d-2ec2-4c5c-ba10-9abcba10799e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filename = f'./models/multiple shapes_' + now\n",
    "# loaded_model = MLP(len(file_paths), 256, 256)\n",
    "# loaded_model.load_state_dict(torch.load(filename))\n",
    "# loaded_model.eval()\n",
    "\n",
    "x = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx = 1\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * (shape_idx)\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a835da5-bcc6-4dc8-a821-9de6fd14bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with ground truth\n",
    "shape_idx = 1\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b0c9b-2a02-47f2-b0a2-cd4b1708e998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filename = f'./models/multiple shapes_' + now\n",
    "# loaded_model = MLP(len(file_paths), 256, 256)\n",
    "# loaded_model.load_state_dict(torch.load(filename))\n",
    "# loaded_model.eval()\n",
    "\n",
    "x = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx = 2\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * (shape_idx)\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cd3146-4af4-46ab-af98-45f457f47e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with ground truth\n",
    "shape_idx = 2\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e50d9-e6d9-4a94-881e-2d8df5d1d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = f'./models/multiple shapes_' + now\n",
    "# loaded_model = MLP(len(file_paths), 256, 256)\n",
    "# loaded_model.load_state_dict(torch.load(filename))\n",
    "# loaded_model.eval()\n",
    "\n",
    "x = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx = 5\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * (shape_idx)\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6a5c9-bb23-458e-9080-e6c55762030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with ground truth\n",
    "shape_idx = 5\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb67e4-39b6-4e0f-8260-f35f8d65d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = f'./models/multiple shapes_' + now\n",
    "# loaded_model = MLP(len(file_paths), 256, 256)\n",
    "# loaded_model.load_state_dict(torch.load(filename))\n",
    "# loaded_model.eval()\n",
    "\n",
    "x = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx = 6\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * (shape_idx)\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef17e1c-aa8d-49f6-993b-67f457bbbc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with ground truth\n",
    "shape_idx = 6\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a147cb4-90dc-428b-bc13-31849c5c6f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = f'./models/multiple shapes_' + now\n",
    "# loaded_model = MLP(len(file_paths), 256, 256)\n",
    "# loaded_model.load_state_dict(torch.load(filename))\n",
    "# loaded_model.eval()\n",
    "\n",
    "x = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 150, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx = 7\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * (shape_idx)\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ea2151-2c6e-4f09-ae62-d9e7b2559825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with ground truth\n",
    "shape_idx = 7\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f56e3-3e47-452f-89a2-23b13036c93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = f'./models/multiple shapes_' + now\n",
    "# loaded_model = MLP(len(file_paths), 256, 256)\n",
    "# loaded_model.load_state_dict(torch.load(filename))\n",
    "# loaded_model.eval()\n",
    "\n",
    "x = np.linspace(-1, 1, 300, dtype=np.float32)\n",
    "y = np.linspace(-1, 1, 300, dtype=np.float32)\n",
    "z = np.linspace(-1, 1, 300, dtype=np.float32)\n",
    "P = np.vstack(np.meshgrid(x,y,z)).reshape(3,-1).T  # format: [[x1, y1, z1], [x1, y1, z2], [] ...]\n",
    "P = torch.from_numpy(P)\n",
    "\n",
    "shape_idx = 9\n",
    "shape_idx_tensor = torch.ones((P.shape[0], 1), dtype=torch.int) * (shape_idx)\n",
    "\n",
    "volume = model(shape_idx_tensor,P).view(len(x), len(y), len(z)) \n",
    "volume = volume.detach().numpy()\n",
    "\n",
    "verts, faces, normals, values = measure.marching_cubes(volume, 0)\n",
    "mp.plot(verts, faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a70b2-8d1c-4a9f-a38c-7a7c8640a113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with ground truth\n",
    "shape_idx = 9\n",
    "mesh = trimesh.load(file_paths[shape_idx].split('sdf_samples.npz')[0] + 'mesh.obj')\n",
    "mp.plot(mesh.vertices, mesh.faces, c=np.array([0, 0.9, 0.9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc69628-d0e7-43ab-8ce4-a374dd210511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b187df13-f368-42eb-a1f5-3f22a104b292",
   "metadata": {},
   "source": [
    "Epoch: 1 \tTraining Loss: 0.000381\n",
    "Epoch: 11 \tTraining Loss: 0.000087\n",
    "Epoch: 21 \tTraining Loss: 0.000082\n",
    "Epoch: 31 \tTraining Loss: 0.000055\n",
    "Epoch: 41 \tTraining Loss: 0.000054\n",
    "Epoch: 51 \tTraining Loss: 0.000046\n",
    "Epoch: 61 \tTraining Loss: 0.000041\n",
    "Epoch: 71 \tTraining Loss: 0.000037\n",
    "Epoch: 81 \tTraining Loss: 0.000029\n",
    "Epoch: 91 \tTraining Loss: 0.000027\n",
    "Epoch: 101 \tTraining Loss: 0.000029\n",
    "Epoch: 111 \tTraining Loss: 0.000025\n",
    "Epoch: 121 \tTraining Loss: 0.000025\n",
    "Epoch: 131 \tTraining Loss: 0.000026\n",
    "Epoch: 141 \tTraining Loss: 0.000022\n",
    "Epoch: 151 \tTraining Loss: 0.000028\n",
    "Epoch: 161 \tTraining Loss: 0.000023\n",
    "Epoch: 171 \tTraining Loss: 0.000020\n",
    "Epoch: 181 \tTraining Loss: 0.000027\n",
    "Epoch: 191 \tTraining Loss: 0.000022\n",
    "Epoch: 201 \tTraining Loss: 0.000017\n",
    "Epoch: 211 \tTraining Loss: 0.000016\n",
    "Epoch: 221 \tTraining Loss: 0.000015\n",
    "Epoch: 231 \tTraining Loss: 0.000030\n",
    "Epoch: 241 \tTraining Loss: 0.000019\n",
    "Epoch: 251 \tTraining Loss: 0.000021\n",
    "Epoch: 261 \tTraining Loss: 0.000017\n",
    "Epoch: 271 \tTraining Loss: 0.000018\n",
    "Epoch: 281 \tTraining Loss: 0.000015\n",
    "Epoch: 291 \tTraining Loss: 0.000017\n",
    "Epoch: 301 \tTraining Loss: 0.000016\n",
    "Epoch: 311 \tTraining Loss: 0.000013\n",
    "Epoch: 321 \tTraining Loss: 0.000012\n",
    "Epoch: 331 \tTraining Loss: 0.000014\n",
    "Epoch: 341 \tTraining Loss: 0.000015\n",
    "Epoch: 351 \tTraining Loss: 0.000011\n",
    "Epoch: 361 \tTraining Loss: 0.000011\n",
    "Epoch: 371 \tTraining Loss: 0.000019\n",
    "Epoch: 381 \tTraining Loss: 0.000015\n",
    "Epoch: 391 \tTraining Loss: 0.000012\n",
    "Epoch: 401 \tTraining Loss: 0.000011\n",
    "Epoch: 411 \tTraining Loss: 0.000015\n",
    "Epoch: 421 \tTraining Loss: 0.000012\n",
    "Epoch: 431 \tTraining Loss: 0.000015\n",
    "Epoch: 441 \tTraining Loss: 0.000010\n",
    "Epoch: 451 \tTraining Loss: 0.000011\n",
    "Epoch: 461 \tTraining Loss: 0.000010\n",
    "Epoch: 471 \tTraining Loss: 0.000015\n",
    "Epoch: 481 \tTraining Loss: 0.000009\n",
    "Epoch: 491 \tTraining Loss: 0.000010\n",
    "Epoch: 501 \tTraining Loss: 0.000010\n",
    "Epoch: 511 \tTraining Loss: 0.000010\n",
    "Epoch: 521 \tTraining Loss: 0.000010\n",
    "Epoch: 531 \tTraining Loss: 0.000009\n",
    "Epoch: 541 \tTraining Loss: 0.000013\n",
    "Epoch: 551 \tTraining Loss: 0.000011\n",
    "Epoch: 561 \tTraining Loss: 0.000009\n",
    "Epoch: 571 \tTraining Loss: 0.000008\n",
    "Epoch: 581 \tTraining Loss: 0.000010\n",
    "Epoch: 591 \tTraining Loss: 0.000008\n",
    "Epoch: 601 \tTraining Loss: 0.000009\n",
    "Epoch: 611 \tTraining Loss: 0.000009\n",
    "Epoch: 621 \tTraining Loss: 0.000009\n",
    "Epoch: 631 \tTraining Loss: 0.000009\n",
    "Epoch: 641 \tTraining Loss: 0.000009\n",
    "Epoch: 651 \tTraining Loss: 0.000010\n",
    "Epoch: 661 \tTraining Loss: 0.000010\n",
    "Epoch: 671 \tTraining Loss: 0.000011\n",
    "Epoch: 681 \tTraining Loss: 0.000010\n",
    "Epoch: 691 \tTraining Loss: 0.000008\n",
    "Epoch: 701 \tTraining Loss: 0.000008\n",
    "Epoch: 711 \tTraining Loss: 0.000009\n",
    "Epoch: 721 \tTraining Loss: 0.000008\n",
    "Epoch: 731 \tTraining Loss: 0.000010\n",
    "Epoch: 741 \tTraining Loss: 0.000008\n",
    "Epoch: 751 \tTraining Loss: 0.000008\n",
    "Epoch: 761 \tTraining Loss: 0.000008\n",
    "Epoch: 771 \tTraining Loss: 0.000008\n",
    "Epoch: 781 \tTraining Loss: 0.000008\n",
    "Epoch: 791 \tTraining Loss: 0.000008\n",
    "Epoch: 801 \tTraining Loss: 0.000009\n",
    "Epoch: 811 \tTraining Loss: 0.000008\n",
    "Epoch: 821 \tTraining Loss: 0.000007\n",
    "Epoch: 831 \tTraining Loss: 0.000008\n",
    "Epoch: 841 \tTraining Loss: 0.000010\n",
    "Epoch: 851 \tTraining Loss: 0.000008\n",
    "Epoch: 861 \tTraining Loss: 0.000008\n",
    "Epoch: 871 \tTraining Loss: 0.000009\n",
    "Epoch: 881 \tTraining Loss: 0.000007\n",
    "Epoch: 891 \tTraining Loss: 0.000009\n",
    "Epoch: 901 \tTraining Loss: 0.000008\n",
    "Epoch: 911 \tTraining Loss: 0.000008\n",
    "Epoch: 921 \tTraining Loss: 0.000008\n",
    "Epoch: 931 \tTraining Loss: 0.000008\n",
    "Epoch: 941 \tTraining Loss: 0.000009\n",
    "Epoch: 951 \tTraining Loss: 0.000013\n",
    "Epoch: 961 \tTraining Loss: 0.000007\n",
    "Epoch: 971 \tTraining Loss: 0.000013\n",
    "Epoch: 981 \tTraining Loss: 0.000008\n",
    "Epoch: 991 \tTraining Loss: 0.000007\n",
    "Epoch: 1001 \tTraining Loss: 0.000007\n",
    "Epoch: 1011 \tTraining Loss: 0.000007\n",
    "Epoch: 1021 \tTraining Loss: 0.000008\n",
    "Epoch: 1031 \tTraining Loss: 0.000007\n",
    "Epoch: 1041 \tTraining Loss: 0.000007\n",
    "Epoch: 1051 \tTraining Loss: 0.000008\n",
    "Epoch: 1061 \tTraining Loss: 0.000006\n",
    "Epoch: 1071 \tTraining Loss: 0.000006\n",
    "Epoch: 1081 \tTraining Loss: 0.000008\n",
    "Epoch: 1091 \tTraining Loss: 0.000007\n",
    "Epoch: 1101 \tTraining Loss: 0.000006\n",
    "Epoch: 1111 \tTraining Loss: 0.000007\n",
    "Epoch: 1121 \tTraining Loss: 0.000007\n",
    "Epoch: 1131 \tTraining Loss: 0.000006\n",
    "Epoch: 1141 \tTraining Loss: 0.000007\n",
    "Epoch: 1151 \tTraining Loss: 0.000006\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
